---
title: Principal Components Analysis
subtitle: I derive the PCA decomposition from both a minimum reconstruction error and maximum variance perspective. I also discuss a statistical interpretation of PCA.
layout: default
date: 2023-12-15
keywords: PCA, Statistics 
published: true
---

## Part 1: Formulating and Solving the PCA Optimization Problem

### Setup and Notation
{% katexmm %}
Suppose that we have data $\mathbf{x}_1, \dots, \mathbf{x}_N \in \mathbb{R}^D$, stacked into the
rows of a matrix $X \in \mathbb{R}^{N \times D}$. Our task is to find a subspace
of smaller dimension $R < D$ such that the projection of the data points onto the
subspace retains as much information as possible. By restricting our attention to
orthonormal bases for the low-dimensional subspace, we reduce the problem to finding
a set of orthonormal basis vectors
\begin{align}
&\mathbf{b}_1, \dots, \mathbf{b}_R \in \mathbb{R}^D,
&&\langle \mathbf{b}_r, \mathbf{b}_s \rangle = \delta\_{r,s}
\end{align}
Define $B \in \mathbb{R}^{D \times R}$ to be the matrix with $r^{\text{th}}$ column
equal to $\mathbf{b}_r$. The subspace generated by the basis $B$ is given by
$$
\text{span}(B) := \text{span}(\mathbf{b}_1, \dots, \mathbf{b}_R).
$$
Throughout this post I will abuse notation by referring to the matrix $B$ when actually
talking about the set of vectors $\{\mathbf{b}_1, \dots, \mathbf{b}_R\}$. Since there
is no a priori reason to assume that the data is centered, we should also allow for
the subspace to be shifted by some intercept $\mathbf{w}_0 \in \mathbb{R}^D$,
resulting in the affine space
$$
\mathbf{w}_0 + \text{span}(B) = \left\{\mathbf{w}_0 +
\sum_{r=1}^{R} w_r \mathbf{b}_r : w_1, \dots, w_R \in \mathbb{R} \right\}.
$$
Loosely speaking, the task is to find the basis
$B$, intercept $\mathbf{w}_0$, and pointwise weights
$\mathbf{w}_1, \dots, \mathbf{w}_N \in \mathbb{R}^R$ such that
\begin{align}
\mathbf{x}_n &\approx \mathbf{w}_0 + \sum\_{r=1}^{R} (\mathbf{w}_n)_r \mathbf{b}_r, &&\forall n=1,\dots,N \newline
&= \mathbf{w}_0 + B\mathbf{w}_n.
\end{align}
To formalize this notion, PCA measures the error in the above approximation using
Euclidean distance, averaged over the $N$ data points. To further sumplify notation,
we stack the $\mathbf{w}_n$ in the columns of a matrix $W \in \mathbb{R}^{R \times N}$.
With all of this notation established, we can state that PCA solves the optimization
problem
$$
\text{argmin}_{B, W, \mathbf{w}_0} \sum_{n=1}^{N} \lVert \mathbf{x}_n - (\mathbf{w}_0 + B\mathbf{w}_n) \rVert_2^2,
$$
where the basis $B$ is constrained to be orthonormal.
As we will see, this optimization naturally breaks down into two distinct problems
which can be solved sequentially:
1. Given the basis $B$ and intercept $\mathbf{w}_0$, find the optimal basis coefficients
$\mathbf{w}_n$ corresponding to each data point $\mathbf{x}_n$.
2. Find the optimal basis and intercept.
Part of the popularity of PCA stems from the fact that both problems can be solved in
closed-form. Let us consider both problems in turn.

### Optimizing the Basis Coefficients
Let us first consider $\mathbf{w}_0$ and $B$ to be fixed, meaning that we are fixing
an affine subspace of dimension $R$. We seek to find the optimal way to represent
the data $X$ in this lower-dimensional space. As we will show, the Euclidean objective
used by PCA implies that this problem reduces to straightforward orthogonal projection.
For now, let $\mathbf{x}^c_n := \mathbf{x}_n - \mathbf{w}_0$ denote the centered
data points (we will deal with the intercept shortly). We are thus considering
the problem
$$
\text{argmin}_{W} \sum_{n=1}^{N} \lVert \mathbf{x}^c_n - B\mathbf{w}_n \rVert_2^2
$$
Observe that $\mathbf{w}_n$ only appears in the $n^{\text{th}}$ term of the sum,
meaning that we can consider each summand independently,  
$$
\text{argmin}_{\mathbf{w}_n} \lVert \mathbf{x}^c_n - B\mathbf{w}_n \rVert_2^2.
$$
In words, we seek the linear combination of the basis vectors $B$ that results
in minimal Euclidean distance from $\mathbf{x}_n$; this is a standard orthogonal
projection problem from linear algebra. Since the basis vectors are orthonormal,
the optimal projection coefficients are given by
\begin{align}
&(\mathbf{w}_n)_r = \langle \mathbf{x}_n, \mathbf{b}_r \rangle,
&&\mathbf{w}_n = B^\top \mathbf{x}_n
\end{align}
which can be written succinctly for all data points as
$$
W^\top = X^c B,
$$
with $X^c$ denoting the centered data matrix with rows set to the
$(\mathbf{x}^c_n)^\top$.
{% endkatexmm %}

### Optimizing the Basis and Intercept
In the previous section, we saw that for a fixed basis and intercept, optimizing
the basis weights reduced to an orthogonal projection problem. In this section
we show that for fixed weights, optimizing the basis reduces to solving a sequence
of eigenvalue problems. To be clear, we are now considering the problem
{% katexmm %}
$$
\text{argmin}_{B} \sum_{n=1}^{N} \lVert \mathbf{x}_n - B\mathbf{w}_n \rVert_2^2,
$$
where we are additionally treating $\mathbf{w}_0$ as fixed for the time being.


{% endkatexmm %}

### An Alternative Approach: The Eckart-Young Theorem
Armed with the Eckart-Young Theorem, we can derive the same result with much less
work. The key is to re-write the reconstruction error as a a matrix approximation
problem using the Frobenius norm. We have

{% katexmm %}
$$
\sum_{n=1}^{N} \lVert \mathbf{x}_n - B\mathbf{w}_n \rVert_2^2
= \lVert X - WB^\top \rVert_2^2,
$$
where the function $\lVert \cdot \rVert_2$ denotes the Frobenius norm when
passed a matrix. The PCA optimization problem can then be written as the
matrix approximation problem
$$
\text{argmin}_{B, W} \lVert X - WB^\top \rVert_2^2,
$$
where $B$ is constrained to be an orthogonal matrix. This is precisely the problem
considered by the Eckart-Young theorem.
{% endkatexmm %}

## Part 2: Interpreting PCA
1. Maximum variance or minimum reconstruction error  
2. Statistical Interpretation
3. View as regression problem where you simultaneously optimize the design matrix


## Part 3: Computing PCA
1. Eigendecomposition
2. SVD


## Part 4: Using PCA
1. Decorrelating
2. Dimensionality reduction

## Part 5: Application and Code


## Appendix

### Eigenvalue Problems
{% katexmm %}
In this section, I briefly discuss the spectral norm and eigenvalue problems in
finite-dimensional vector spaces, which I utilize above when optimizing the basis
$B$ in the PCA derivation. Consider vector spaces $\mathcal{V} \subseteq \mathbb{R}^N$
and $\mathcal{U} \subseteq \mathbb{R}^D$, equipped with the standard Euclidean
inner product. Consider an arbitrary matrix $A \in \mathbb{R}^{N \times D}$, which
defines a linear map from $\mathcal{U}$ to $\mathcal{V}$. We define the
**spectral norm** of $A$ as the largest factor by which the map
$A$ can "stretch" a vector $\mathbf{u} \in \mathcal{U}$
$$
\lVert A \rVert_2 := \max_{u \in \mathcal{U}}
\frac{\lVert A\mathbf{u} \rVert_{\mathcal{V}}}{\lVert \mathbf{u} \rVert_{\mathcal{U}}},
$$
where I write $\lVert \cdot \rVert_{\mathcal{U}}$ to denote the Euclidean norm on
$\mathcal{U}$, and similarly for $\lVert \cdot \rVert_{\mathcal{V}}$, as it will
be helpful to emphasize in which space the operation is being applied. Using the
linearity of $A$, one can show that we need only consider vectors of unit length;
that is,
$$
\lVert A \rVert_2 = \max_{\lVert \mathbf{u} \rVert_{\mathcal{U}}=1} \lVert A\mathbf{u} \rVert_{\mathcal{V}}.
$$
I claim that $\lVert A \rVert_2$ is given by the square root of the
eigenvalue of $A$ with the largest magnitude.
Recalling the convention that all eigenvalues will be sorted in decreasing
order of their magnitude, my claim is
$$
\lVert A \rVert_2 = \lambda_1(A^\top A)^{1/2}.
$$
Note that this only makes sense if $A^\top A$ is guaranteed to have eigenvalues.
Since $A^\top A$ is symmetric, this is indeed justified by the spectral theorem,
which guarantees that $A^\top A$ has $D$ real eigenvalues
$\lambda_1, \dots, \lambda_D$ with an associated set of orthonormal eigenvectors
$\mathbf{e}_1, \dots, \mathbf{e}_D$ forming a basis of $\mathcal{U}$. We will
utilize this orthonormal basis in proving the claim.

To this end, let $\mathbf{u} \in \mathcal{U}$
be an arbitrary vector. Since the eigenvectors form a basis of $\mathcal{U}$, then
$\mathbf{u}$ can be represented as
$$
\mathbf{u} = \sum_{d=1}^{D} u_d \mathbf{e}_d
$$
for some scalars $u_1, \dots, u_D$. We will use this representation to show that
1. $\lVert A\mathbf{u} \rVert$ is upper bounded by $\lambda_1^{1/2}$.
2. The upper bound is achieved by some $\mathbf{u} \in \mathcal{U}$.

These two facts together imply the claim. We will actually work with the squared
norm instead, which allows us to leverage the inner product. We have
\begin{align}
\lVert A\mathbf{u} \rVert^2_{\mathcal{V}}
&= \langle A\mathbf{u}, A\mathbf{u} \rangle\_{\mathcal{V}} \newline
&= \langle A^\top A\mathbf{u}, \mathbf{u} \rangle\_{\mathcal{V}} \newline
&= \left\langle A^\top A \sum\_{d=1}^{D} u_d \mathbf{e}_d,
\sum\_{d=1}^{D} u_d \mathbf{e}_d \right\rangle\_{\mathcal{V}} \newline
&= \left\langle \sum\_{d=1}^{D} u_d (A^\top A \mathbf{e}_d),
\sum\_{d=1}^{D} u_d \mathbf{e}_d \right\rangle\_{\mathcal{V}} \newline
&= \left\langle \sum\_{d=1}^{D} u_d \lambda_d \mathbf{e}_d,
\sum\_{d=1}^{D} u_d \mathbf{e}_d \right\rangle\_{\mathcal{V}} \newline
&= \sum\_{d=1}^{D} u_d^2 \lambda_d \lVert \mathbf{e}_d \rVert^2\_{\mathcal{V}} \newline
&= \sum\_{d=1}^{D} u_d^2 \lambda_d \newline
&\leq \sum\_{d=1}^{D} u_d^2 \lvert \lambda_1 \rvert \newline
&= \lvert \lambda_1 \rvert \lVert \mathbf{u} \rVert\_{\mathcal{V}},
\end{align}
where the sixth and seventh inequalities follow from the orthonormality of the
$\mathbf{e}_d$. Restricting to vectors $\mathbf{u}$ of unit norm, we have found
$$
\lVert A\mathbf{u} \rVert_{\mathcal{V}} \leq \sqrt{\lambda_1}.
$$
It remains to show that this upper bound is achieved by some
$\mathbf{u} \in \mathcal{U}$.
{% endkatexmm %}
